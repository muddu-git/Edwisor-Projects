#Importing Libraries
import pandas as pd
import numpy as np

#reading data
data=pd.read_excel("C:/Users/mudmoham/Documents/pr/case studies/Employee Absenteeism/Absenteeism_at_work_Project.xls",sheetname="Year_Sheet")

#Missing Value Analysis
data["Reason for absence"]=data["Reason for absence"].fillna(20)
data=data[data["Reason for absence"]!=0]
data["Service time"]=data["Service time"].replace(29,19)

#KNN Imputation
from fancyimpute import KNN
data=pd.DataFrame(KNN(k=3).complete(data),columns=data.columns)
data=data.apply(np.round,axis=1)

cat_columns=['ID','Reason for absence','Month of absence','Day of the week','Seasons','Disciplinary failure','Education','Son','Social drinker','Social smoker','Pet','Absenteeism time in hours']
num_columns=['Transportation expense','Distance from Residence to Work','Age','Work load Average/day ','Hit target','Weight','Height','Body mass index']

#Outlier Analysis through KNN
for col in num_columns:
	q75,q25=np.percentile(data[col],[75,25])
	iqr=q75-q25
	maximum=q75+iqr*1.5
	minimum=q25-iqr*1.5
	data.loc[data[col]<minimum,col]=np.nan
	data.loc[data[col]>maximum,col]=np.nan
	
data=pd.DataFrame(KNN(k=3).complete(data),columns=data.columns)
data=data.apply(np.round,axis=1)

#Feature Engineering
data["BMI"]=pd.cut(data["Body mass index"],[0,18.5,24.9,29.9,40],labels=["underweight","normal weight","overweight","obesity"])
data["Age"]=pd.cut(data["Age"],[17,34,55,80],labels=["Young Adults","Middle Aged","Old"])
data["Education"]=data["Education"].replace({1:"High School",2:"Graduate",3:"Post Graduate",4:"Master and Doctor"})
data["Son"]=pd.cut(data["Son"],[0.0,0.99,2.99,4],labels=["No Children","Less Child Count","More Child Count"])
data["Son"]=data["Son"].fillna("No Children")
data["Pet"]=pd.cut(data["Pet"],[0.0,0.99,2.99,8],labels=["No Pets","Less Pets","More Pets"])
data["Pet"]=data["Pet"].fillna("No Pets")
data["Service time"]=pd.cut(data["Service time"],[1.0,3.99,9.99,15.99,21.99,24],labels=["Mid-Night","Morning","Noon","Evening","Night"])
data["Service time"]=data["Service time"].fillna("Mid-Night")
data["Day of the week"]=data["Day of the week"].replace({2:"Monday",3:"Tuesday",4:"Wednesday",5:"Thursday",6:"Friday"})
data["Seasons"]=data["Seasons"].replace({1:"Summer",2:"Autumn",3:"Winter",4:"Spring"})
di={1:"infectious diseases",2:"Cancer Related",3:"Immune Related",4:"Nutrional Related",5:"Mental Disorders",6:"Mental Disorders",7:"Eye and Ear",8:"Eye and Ear",9:"Heart Related",10:"Respiratory",11:"Digestive",12:"Skin",13:"Physical Issues",14:"genitourinary",15:"Maternity,Paternity",16:"Abnormal Diseases",17:"Abnormal Diseases",18:"Abnormal Diseases",19:"Injury,Poisoning",20:"External Factors",21:"External Factors",22:"Follow Up",23:"Consultations",27:"Consultations",28:"Consultations",24:"Blood Donations",25:"Lab",26:"Unjustified Absence"}
data["Reason for absence"]=data["Reason for absence"].replace(di)
data["ID"]=data["ID"].replace([2,6,7,12,16,19,21,23,25,26,27,29,30,31,32,33],"Less Absentees")
data["ID"]=data["ID"].replace([1,5,9,10,13,15,17,18,22,24],"Medium Absentees")
data["ID"]=data["ID"].replace([3,11,14,20,28,34,36],"More Absentees")

#Dummy Encoding
data_encode=pd.get_dummies(data.iloc[:,[0,1,3,5,8,9,13,14,17,22]],drop_first=True)
data_encode1=pd.concat([data,data_encode],axis=1)

#Feature Selection-Dropping Unnecessary Columns Height,Weight,Disciplinary Failure and Social Smoker
data_encode1=data_encode1.drop(data.columns[[0,1,3,5,8,9,12,13,14,16,17,18,19,20,22]],axis=1)

#Grouping Data by Year and Months
data_pre=pd.DataFrame(data_encode1.groupby(["Year","Month of absence"],as_index=False).mean())


#Divinding into train and test
train=data_pre[round(data_pre["Year"])!=2010]
test=data_pre[round(data_pre["Year"])==2010]
xtrain,ytrain=train.drop(["Absenteeism time in hours"],axis=1),train["Absenteeism time in hours"]
xtest,ytest=test.drop(["Absenteeism time in hours"],axis=1),test["Absenteeism time in hours"]

#Feature Scaling
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
xtrain_scaled=scaler.fit_transform(xtrain)
xtest_scaled=scaler.fit_transform(xtest)

from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.cross_validation import cross_val_score
rf=RandomForestRegressor()
rf.fit(xtrain,ytrain)
scores=cross_val_score(rf,xtrain,ytrain,scoring='mean_squared_error')
rf.score(xtest,ytest)
y_pred=rf.predict(xtest)

from sklearn.metrics import mean_squared_error
print(mean_squared_error(ytest,y_pred))



def fitting_model(model_p,param,xtrain_scaled,ytrain,xtest_scaled,ytest):
	gs_model=GridSearchCV(model_p,cv=5,param_grid=param,scoring='mean_squared_error')
	gs_model.fit(xtrain_scaled,ytrain)
	best_est=gs_model.best_estimator_
	print(best_est)
	print("*********CV Score**********************")
	print(gs_model.best_score_)
	y_pred=best_est.predict(xtest_scaled)
	print("*********mean_squared_error**************")
	print(mean_squared_error(ytest,y_pred))
	print("*********RMSE*****************************")
	print(np.sqrt(mean_squared_error(ytest,y_pred)))
	
from sklearn.ensemble import RandomForestRegressor 
param={"n_estimators":[10,100,500],"min_samples_split":[2,5,10],"min_samples_leaf":[1,5,10],"min_weight_fraction_leaf":[0,0.5]} 
from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error
from sklearn.model_selection import GridSearchCV

fitting_model(RandomForestRegressor(),param,xtrain_scaled,ytrain,xtest_scaled,ytest)


from sklearn.ensemble import GradientBoostingRegressor
param = {"n_estimators": [10,30,100,200,500],
               "max_depth": [None,5,10,15,20,50],
               "min_samples_split": [3,5,10,20],
               "min_samples_leaf": [1,10,20,50],
               "max_leaf_nodes": [None,5,10,20],
			   }
fitting_model(GradientBoostingRegressor(random_state=0),param,xtrain_scaled,ytrain,xtest_scaled,ytest)
               
from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
param={"min_samples_split":[2,5,10],'min_samples_leaf':[1,5,10],'max_features':['auto','sqrt','log2'],'min_weight_fraction_leaf':[0,0.5]} 	
fitting_model(DecisionTreeRegressor(random_state=0),param,xtrain_scaled,ytrain,xtest_scaled,ytest)


from sklearn.neighbors import KNeighborsRegressor
param={'n_neighbors':[3,5,7,9,11,13,15],'weights':['uniform','distance'],'p':[1,2]}
fitting_model(KNeighborsRegressor(),param,xtrain_scaled,ytrain,xtest_scaled,ytest)


from sklearn.svm import SVR
param={'C':list(np.arange(0.5,10,0.5)),'gamma':list(np.arange(0.5,10,0.5))}
fitting_model(SVR(),param,xtrain_scaled,ytrain,xtest_scaled,ytest)


               


